#!/usr/bin/env python3

import json
import boto3
import time
from datetime import datetime

# Sample resume text (based on your original resume)
SAMPLE_RESUME = """
ABID SHAIK
(716) 970-9249 | abidshariff009@gmail.com | Dallas, Texas | LinkedIn

PROFESSIONAL SUMMARY
Experienced Data Engineer with 10+ years of experience building scalable data platforms and analytics solutions. Skilled in cloud technologies, big data processing, and data pipeline development.

CORE COMPETENCIES
AWS ‚Ä¢ Python ‚Ä¢ SQL ‚Ä¢ Apache Spark ‚Ä¢ Data Engineering ‚Ä¢ ETL/ELT ‚Ä¢ Database Management

PROFESSIONAL EXPERIENCE

Data Engineer | Amazon
Dec '20 ‚Äî Present
‚Ä¢ Built AI assistant using AWS Bedrock integrating data from Redshift and DynamoDB
‚Ä¢ Designed real-time analytics solutions for workload management
‚Ä¢ Developed centralized metrics library with 200+ core recruiting metrics
‚Ä¢ Led cross-functional team for case management tool development
‚Ä¢ Implemented automation framework reducing pipeline development time by 70%

Data Engineer | CGI  
Jun '15 ‚Äî Dec '20
‚Ä¢ Designed AWS systems following compliance and security standards
‚Ä¢ Built scalable, fault-tolerant environments enhancing uptime by 30%
‚Ä¢ Led cloud migration reducing infrastructure costs by 25%
‚Ä¢ Developed centralized data repositories and automated workflows

ETL Analyst | Tech Mahindra
Feb '13 ‚Äî Jul '14
‚Ä¢ Built decision support model reducing insurance claims cost by 35%
‚Ä¢ Published interactive reports using Tableau Server
‚Ä¢ Developed scoring model for de-duplication of 50M+ records
‚Ä¢ Designed data repository for investment bank analytics

Systems Engineer | Infosys
Oct '10 ‚Äî Oct '12
‚Ä¢ Managed financial transactions for multinational bank
‚Ä¢ Designed OBIEE dashboards with drill-downs and navigation
‚Ä¢ Developed Unix shell scripts optimizing execution time by 70%
‚Ä¢ Automated production processes reducing manual effort by 65%

EDUCATION
Masters in Information Systems | University at Buffalo (GPA: 3.89)
Bachelors in Computer Science | Vellore Institute of Technology (GPA: 3.7)
"""

# 5 Different job descriptions to test
JOB_DESCRIPTIONS = {
    "snowflake_data_engineer": """
    Senior Data Engineer - Snowflake Platform
    
    We're seeking a Senior Data Engineer to join our data platform team building next-generation analytics infrastructure on Snowflake.
    
    Responsibilities:
    ‚Ä¢ Design and implement scalable data pipelines using Snowflake, DBT, and Fivetran
    ‚Ä¢ Build ELT workflows for data transformation and modeling
    ‚Ä¢ Implement Snowpipe for real-time data ingestion
    ‚Ä¢ Collaborate with analytics teams on data warehouse design
    ‚Ä¢ Optimize query performance and cost management in Snowflake
    
    Requirements:
    ‚Ä¢ 5+ years of data engineering experience
    ‚Ä¢ Expert-level Snowflake knowledge
    ‚Ä¢ Strong experience with DBT for data transformation
    ‚Ä¢ Experience with Fivetran or similar data integration tools
    ‚Ä¢ Knowledge of Snowpipe for streaming data ingestion
    ‚Ä¢ SQL expertise and data modeling skills
    ‚Ä¢ Python programming for data processing
    """,
    
    "streaming_data_engineer": """
    Principal Data Engineer - Real-time Analytics
    
    Join our team building the next generation of real-time data processing systems for millions of users.
    
    What You'll Do:
    ‚Ä¢ Build fault-tolerant streaming data pipelines using Apache Kafka and Apache Flink
    ‚Ä¢ Design real-time analytics using Apache Spark Streaming
    ‚Ä¢ Implement event-driven architectures with Kafka Connect
    ‚Ä¢ Work with Apache Airflow for workflow orchestration
    ‚Ä¢ Build monitoring and alerting for streaming systems
    
    Requirements:
    ‚Ä¢ 7+ years in data engineering with focus on streaming
    ‚Ä¢ Expert knowledge of Apache Kafka and Apache Flink
    ‚Ä¢ Strong experience with Apache Spark for batch and streaming
    ‚Ä¢ Experience with Apache Airflow for orchestration
    ‚Ä¢ Proficiency in Scala, Python, and Java
    ‚Ä¢ Knowledge of distributed systems and microservices
    ‚Ä¢ Experience with AWS or GCP for cloud infrastructure
    """,
    
    "ml_data_engineer": """
    Senior ML Data Engineer
    
    We're looking for a Senior ML Data Engineer to build data infrastructure supporting our machine learning platform.
    
    Key Responsibilities:
    ‚Ä¢ Build ML data pipelines using Apache Airflow and Kubeflow
    ‚Ä¢ Implement feature stores and data versioning systems
    ‚Ä¢ Work with MLflow for model lifecycle management
    ‚Ä¢ Deploy models using TensorFlow Serving and PyTorch
    ‚Ä¢ Build data quality monitoring and validation systems
    ‚Ä¢ Collaborate with data scientists on model deployment
    
    Required Skills:
    ‚Ä¢ 5+ years of data engineering experience in ML environments
    ‚Ä¢ Strong Python programming and data processing skills
    ‚Ä¢ Experience with ML frameworks: TensorFlow, PyTorch, Scikit-learn
    ‚Ä¢ Knowledge of MLOps tools: MLflow, Kubeflow, Apache Airflow
    ‚Ä¢ Experience with containerization: Docker, Kubernetes
    ‚Ä¢ Understanding of feature engineering and model serving
    ‚Ä¢ Cloud platform experience (AWS, GCP, Azure)
    """,
    
    "aws_data_engineer": """
    Senior AWS Data Engineer
    
    Join our cloud data team building enterprise-scale data solutions on AWS.
    
    What You'll Build:
    ‚Ä¢ Serverless data pipelines using AWS Lambda and Step Functions
    ‚Ä¢ Data lakes using AWS S3, Glue, and Athena
    ‚Ä¢ Real-time streaming with AWS Kinesis and Kinesis Analytics
    ‚Ä¢ Data warehousing solutions with Amazon Redshift
    ‚Ä¢ ETL processes using AWS Glue and EMR
    ‚Ä¢ Monitoring and alerting with CloudWatch and SNS
    
    Requirements:
    ‚Ä¢ 6+ years of data engineering experience
    ‚Ä¢ Deep AWS expertise: S3, Lambda, Glue, EMR, Redshift, Kinesis
    ‚Ä¢ Strong Python and SQL programming skills
    ‚Ä¢ Experience with Infrastructure as Code (CloudFormation, Terraform)
    ‚Ä¢ Knowledge of data governance and security best practices
    ‚Ä¢ Experience with CI/CD pipelines and DevOps practices
    ‚Ä¢ AWS certifications preferred
    """,
    
    "analytics_engineer": """
    Senior Analytics Engineer
    
    We're seeking an Analytics Engineer to build self-service analytics infrastructure and empower data-driven decision making.
    
    Responsibilities:
    ‚Ä¢ Build and maintain data models using DBT and SQL
    ‚Ä¢ Create self-service analytics dashboards with Tableau and Looker
    ‚Ä¢ Implement data quality testing and monitoring
    ‚Ä¢ Design dimensional data models for business intelligence
    ‚Ä¢ Build automated reporting and alerting systems
    ‚Ä¢ Collaborate with business stakeholders on analytics requirements
    
    Required Experience:
    ‚Ä¢ 4+ years in analytics engineering or business intelligence
    ‚Ä¢ Expert SQL skills and data modeling expertise
    ‚Ä¢ Strong experience with DBT for data transformation
    ‚Ä¢ Proficiency with BI tools: Tableau, Looker, or Power BI
    ‚Ä¢ Knowledge of data warehousing concepts and dimensional modeling
    ‚Ä¢ Experience with Python for data analysis and automation
    ‚Ä¢ Understanding of statistical analysis and A/B testing
    """
}

def test_lambda_with_job_descriptions():
    """Test the Lambda function with different job descriptions"""
    
    lambda_client = boto3.client('lambda', region_name='us-east-1')
    results = {}
    
    print("üöÄ Testing AIHandler Lambda with 5 Different Job Descriptions")
    print("=" * 80)
    
    for job_name, job_desc in JOB_DESCRIPTIONS.items():
        print(f"\nüìã Testing: {job_name.upper().replace('_', ' ')}")
        print("-" * 60)
        
        # Create test payload
        test_payload = {
            "resume_text": SAMPLE_RESUME,
            "job_description": job_desc,
            "user_id": "test_user",
            "request_id": f"test_{job_name}_{int(time.time())}"
        }
        
        try:
            print("‚è≥ Invoking Lambda function...")
            start_time = time.time()
            
            response = lambda_client.invoke(
                FunctionName='ResumeOptimizerAIHandler-prod',
                InvocationType='RequestResponse',
                Payload=json.dumps(test_payload)
            )
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # Parse response
            response_payload = json.loads(response['Payload'].read())
            
            if response.get('StatusCode') == 200:
                print(f"‚úÖ Lambda execution successful ({execution_time:.1f}s)")
                
                # Parse the response body
                if 'body' in response_payload:
                    body = json.loads(response_payload['body'])
                    
                    if 'optimizedResume' in body:
                        optimized_resume = body['optimizedResume']
                        results[job_name] = {
                            'success': True,
                            'resume': optimized_resume,
                            'execution_time': execution_time,
                            'model_used': body.get('model_used', 'Unknown')
                        }
                        
                        print(f"üìÑ Resume generated successfully")
                        print(f"ü§ñ Model used: {body.get('model_used', 'Unknown')}")
                        print(f"üìä Resume length: {len(optimized_resume)} characters")
                        
                    else:
                        print(f"‚ùå No optimized resume in response: {body}")
                        results[job_name] = {'success': False, 'error': 'No optimized resume'}
                        
                else:
                    print(f"‚ùå No body in response: {response_payload}")
                    results[job_name] = {'success': False, 'error': 'No response body'}
                    
            else:
                print(f"‚ùå Lambda execution failed: {response_payload}")
                results[job_name] = {'success': False, 'error': response_payload}
                
        except Exception as e:
            print(f"‚ùå Error invoking Lambda: {str(e)}")
            results[job_name] = {'success': False, 'error': str(e)}
        
        # Add delay between requests
        time.sleep(2)
    
    return results

def analyze_resume_outputs(results):
    """Analyze the generated resume outputs"""
    
    print(f"\n\nüîç ANALYSIS OF GENERATED RESUMES")
    print("=" * 80)
    
    successful_results = {k: v for k, v in results.items() if v.get('success')}
    
    print(f"üìä Success Rate: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results)*100:.1f}%)")
    
    if not successful_results:
        print("‚ùå No successful results to analyze")
        return
    
    for job_name, result in successful_results.items():
        print(f"\n{'='*60}")
        print(f"üìã JOB: {job_name.upper().replace('_', ' ')}")
        print(f"{'='*60}")
        
        resume_text = result['resume']
        
        # Extract skills section
        skills_match = extract_section(resume_text, "CORE COMPETENCIES", "PROFESSIONAL EXPERIENCE")
        if skills_match:
            skills = [skill.strip() for skill in skills_match.replace('‚Ä¢', ',').split(',') if skill.strip()]
            print(f"üõ†Ô∏è  SKILLS EXTRACTED ({len(skills)}):")
            for i, skill in enumerate(skills[:10], 1):  # Show first 10
                print(f"   {i:2d}. {skill}")
            if len(skills) > 10:
                print(f"   ... and {len(skills) - 10} more")
        
        # Extract professional summary
        summary_match = extract_section(resume_text, "PROFESSIONAL SUMMARY", "CORE COMPETENCIES")
        if summary_match:
            print(f"\nüìù PROFESSIONAL SUMMARY:")
            print(f"   {summary_match.strip()}")
        
        # Analyze first experience bullet
        exp_match = extract_section(resume_text, "Data Engineer | Amazon", "Data Engineer | CGI")
        if exp_match:
            bullets = [line.strip() for line in exp_match.split('\n') if line.strip().startswith('‚Ä¢')]
            if bullets:
                print(f"\nüíº FIRST EXPERIENCE BULLETS (showing first 3):")
                for i, bullet in enumerate(bullets[:3], 1):
                    print(f"   {i}. {bullet}")
        
        # Check for job-specific technologies
        job_specific_analysis(job_name, resume_text)
        
        print(f"\n‚è±Ô∏è  Execution Time: {result['execution_time']:.1f}s")
        print(f"ü§ñ Model Used: {result['model_used']}")

def extract_section(text, start_marker, end_marker):
    """Extract text between two markers"""
    try:
        start_idx = text.find(start_marker)
        if start_idx == -1:
            return None
        
        start_idx = text.find('\n', start_idx) + 1
        end_idx = text.find(end_marker, start_idx)
        
        if end_idx == -1:
            return text[start_idx:].strip()
        else:
            return text[start_idx:end_idx].strip()
    except:
        return None

def job_specific_analysis(job_name, resume_text):
    """Analyze if resume contains job-specific technologies"""
    
    expected_techs = {
        "snowflake_data_engineer": ["Snowflake", "DBT", "Fivetran", "Snowpipe", "ELT"],
        "streaming_data_engineer": ["Kafka", "Flink", "Spark", "Airflow", "Streaming"],
        "ml_data_engineer": ["MLflow", "TensorFlow", "PyTorch", "Kubeflow", "Machine Learning"],
        "aws_data_engineer": ["AWS", "Lambda", "S3", "Glue", "Redshift", "Kinesis"],
        "analytics_engineer": ["DBT", "Tableau", "Looker", "SQL", "Analytics", "BI"]
    }
    
    if job_name in expected_techs:
        expected = expected_techs[job_name]
        found = []
        missing = []
        
        resume_lower = resume_text.lower()
        for tech in expected:
            if tech.lower() in resume_lower:
                found.append(tech)
            else:
                missing.append(tech)
        
        print(f"\nüéØ JOB-SPECIFIC TECHNOLOGY ANALYSIS:")
        print(f"   Expected: {expected}")
        print(f"   ‚úÖ Found: {found} ({len(found)}/{len(expected)})")
        if missing:
            print(f"   ‚ùå Missing: {missing}")
        
        accuracy = len(found) / len(expected) * 100
        print(f"   üìä Accuracy: {accuracy:.1f}%")

def save_results_to_file(results):
    """Save detailed results to a file for further analysis"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"lambda_test_results_{timestamp}.json"
    
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüíæ Detailed results saved to: {filename}")

if __name__ == "__main__":
    print("üß™ Testing AIHandler Lambda with Multiple Job Descriptions")
    
    # Test Lambda with different job descriptions
    results = test_lambda_with_job_descriptions()
    
    # Analyze the outputs
    analyze_resume_outputs(results)
    
    # Save results for detailed analysis
    save_results_to_file(results)
    
    print(f"\n‚úÖ Testing completed!")
